#!/usr/bin/env python
# coding: utf-8

import json
import os
import sys
import gin
import random
from collections import defaultdict
from datasets import Dataset
import faiss
import time
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
import string
from transformers import AutoTokenizer, AutoModel
import torch
import pandas as pd
import faiss.contrib.torch_utils

# pip install -U faiss-cpu, scikit-learn, sentence-transformers
# python3 generate_intent.py --input=/home/sean/src/dstc8-schema-guided-dialogue/train/ --output=./res/train


@gin.configurable
class SgdDataset:
    def __init__(self, input, output,
                 stop_word_path,  # output path to save generated intent model examples'
                 fix_random_seed=True,  # 'use fixed random seed(for debug)'
                 pos_num=-1,  # 'num of positive sample num generated by each intent'
                 neg_num=-1,  # 'num of negative sample num generated by each intent'
                 training_percentage=1.0,  # 'percentage for training'
                 negative_proportions=1.0,  # 'how many negative examples to generate for each positive example'
                 dev_percentage=1.0,  # 'percentage for dev'
                 decode_method='transformer',  # 'embedding method for the sentence'
                 cover_filter=False,  # 'whether we use the cover_relation to filter the sentence'
                 random_generate=True):  # 'generating the utterance by replacing the slot name with slot val
        self.input = input
        self.output = output
        self.stop_word_path = stop_word_path
        self.fix_random_seed = True
        self.pos_num = pos_num
        self.neg_num = neg_num
        self.training_percentage = training_percentage
        self.negative_proportions = negative_proportions
        self.dev_percentage = dev_percentage
        self.decode_method = decode_method
        self.cover_filter = cover_filter
        self.random_generate = random_generate

    def label(self, pmodel="intents"):
        return f"{pmodel}_{self.decode_method}_random_{self.random_generate}_cover_{self.cover_filter}_pos_{self.pos_num}_neg_{self.neg_num}"


model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')

MODEL = "intents"
TRAIN = "train"
TEST = "test"
DEV = "dev"


# This load the schema.
def load_description_dataset(base_path):
    intent_desc = defaultdict(list)
    with open(base_path + 'schema.json', encoding='utf-8') as f:
        f = json.load(f)

        for service in f:
            # in the "overall"   generate mode,only pick <service>_1  if there are multiple  services for one intent
            if service["service_name"][-1] != '1':
                continue
            service_name = service["service_name"]
            intents = service["intents"]
            for intent in intents:
                intent_name = intent['name']
                if intent_name in intent_desc.keys():
                    print(f"{intent_name} is repeated in {service_name}")
                intent_desc[intent_name] = intent["description"]
    results = pd.DataFrame({"source": intent_desc.keys(), "text": intent_desc.values()})
    results = Dataset.from_pandas(results)
    results = results.map(
        lambda x: {"embeddings": torch.nn.functional.normalize(encoder.convert([x["text"]])).detach().cpu().numpy()[0]}
    )

    results.add_faiss_index(column="embeddings")
    return results


def check_stop_words(slot_dict, utterance, string_list, stop_words_path):
    stop_words = []
    with open(stop_words_path, encoding='utf-8') as f:
        items = f.readlines()
        for t in items:
            stop_words.append(t.lower()[:-1])
    stop_words = set(stop_words)
    # print("stop_words",stop_words)
    single_dict = dict()
    if string_list:
        for key, values in slot_dict.items():
            for value in values:
                single_dict[value] = key
        string_list = sorted(string_list, key=lambda x: x[0])
        res_utterance = utterance[:string_list[0][0]]
        for i, (cur_start, cur_end) in enumerate(string_list):

            if i == len(string_list) - 1:
                res_utterance = res_utterance + utterance[cur_end:]
            else:
                res_utterance = res_utterance + utterance[cur_end:string_list[i + 1][0]]

    else:
        res_utterance = utterance
    punctuation_string = string.punctuation
    for i in punctuation_string:
        res_utterance = res_utterance.replace(i, '')

    all_not_slot_words = set(res_utterance.split())

    if len(all_not_slot_words - stop_words) >= 2:
        return True
    return False


def generate_expression_template(slot_dict, utterance, string_list):
    '''
    replacing the slot val with the slot name,to avoid match the short slot val which may be included in other
    long slot val,we need sort by the length of the slot val
    '''
    if string_list == []:
        return utterance
    single_dict = dict()

    for key, values in slot_dict.items():
        for value in values:
            single_dict[value] = key

    string_list = sorted(string_list, key=lambda x: x[0])
    res_utterance = utterance[:string_list[0][0]]
    for i, (cur_start, cur_end) in enumerate(string_list):
        # if len(string_list) >=2:
        #     print("sub string",utterance[cur_start:cur_end])
        res_utterance = res_utterance + ' < ' + single_dict[utterance[cur_start:cur_end]] + ' > '
        if i == len(string_list) - 1:
            res_utterance = res_utterance + utterance[cur_end:]
        else:
            res_utterance = res_utterance + utterance[cur_end:string_list[i + 1][0]]

    return res_utterance


class IntentMeta:
    """
    restore the all template of a certain intents, including the set of all possible exemplars,
    and the dict for all slot
    """

    def __init__(self):
        self.exemplars = set()
        self.slot_dict = defaultdict(set)
        self.dataset = None
        self.expressions = []

    def add_sample(self, expression):
        expression_template = generate_expression_template(expression.slots, expression.utterance, expression.string_list)
        if expression_template in self.exemplars:
            return

        self.exemplars.add(expression_template)
        expression.exemplar = expression_template

        for slot_name, slot_val_list in expression.slots.items():
            for slot_val in slot_val_list:
                self.slot_dict[slot_name].add(slot_val)

        self.expressions.append(expression)

    def generate_utterance(self, expression):
        expression_template = generate_expression_template(expression.slots, expression.utterance, expression.string_list)
        for slot_name, slot_vals in self.slot_dict.items():
            if '< ' + slot_name + ' >' in expression_template:
                expression_template = expression_template.replace('< ' + slot_name + ' >', list(slot_vals)[random.randint(0, len(slot_vals) - 1)])
        return expression_template

    def finalize(self):
        source = []
        exemplars_list = []

        for key in templates.keys():
            exemplars = templates[key]
            for exemplar in exemplars.exemplars:
                source.append(key)
                exemplars_list.append(exemplar)

        results = pd.DataFrame({"source": source, "text": exemplars_list})
        results = Dataset.from_pandas(results)
        results = results.map(
            lambda x: {"embeddings": torch.nn.functional.normalize(encoder.convert([x["text"]])).detach().cpu().numpy()[0]}
        )

        results.add_faiss_index(column="embeddings")
        self.dataset = results

    def show(self):
        print(self.exemplars)
        print(self.slot_dict)


class Expression:
    """
    expression examples
    """

    def __init__(self, expression, intent, slots, string_list=None):
        self.utterance = expression
        self.intent = intent
        self.slots = slots  # dict to store slot, value pairs
        self.idx = None
        self.string_list = string_list
        self.exemplar = None

    def replace_label(self):
        no_underscore_utterance = self.exemplar
        for key, values in self.slots.items():
            no_underscore_utterance = no_underscore_utterance.replace(key, ' '.join(key.split('_')))
        return no_underscore_utterance

def cover_reaction(expression_A, expression_B):
    '''
    check if the slot of A could cover all of slot of B
    '''
    return set(expression_B.slots.keys()).issubset(set(expression_A.slots.keys()))


def slot_val_to_slot_name(slot_dict, utterance):
    '''
    replacing the slot val with the slot name,to avoid match the short val which may be included
    in other long val,we need sort by the length of the slot val
    '''
    single_dict = dict()

    for key, values in slot_dict.items():
        for value in values:
            single_dict[value] = key

    single_dict = sorted(single_dict.items(), key=lambda x: len(x[0]), reverse=True)

    for (value, key) in single_dict:
        utterance = utterance.replace(value, '< ' + ' '.join(key.split('_')) + ' >')

    return utterance


def load_intent_meta(base_path):
    """
    load original sgd data and create expression examples
    :param base_path: input path to original sgd dataset
    :return: expression examples
    """
    intent_templates = defaultdict(IntentMeta)
    files = os.listdir(base_path)
    sentence_set = defaultdict(set)
    for file in files:
        if file[:6] != 'dialog':
            continue
        with open(base_path + file, encoding='utf-8') as f:
            f = json.load(f)
            for dialogue in f:
                turns = dialogue["turns"]
                pre_intents = set()
                for idx, turn in enumerate(turns):
                    if turn['speaker'] != 'USER':
                        continue
                    active_intents = set()
                    for frame in turn['frames']:
                        active_intents.add(frame['state']['active_intent'])

                    if idx - 1 >= 0 and turns[idx - 1]["frames"][0]["actions"][0]["act"] == "OFFER_INTENT":
                        check_intent = set(turns[idx - 1]["frames"][0]["actions"][0]["values"])
                    else:
                        check_intent = set()

                    if not (active_intents - pre_intents or (not pre_intents)):
                        continue

                    frame = turn['frames'][0]
                    if frame['service'][-1] == '1' and (
                            frame['state']['active_intent'] in active_intents - pre_intents) and \
                            frame['state']['active_intent'] != 'NONE' and \
                            frame['state']['active_intent'] not in check_intent:
                        string_list = []
                        utterance_slot = defaultdict(list)
                        for _slot in frame['slots']:
                            utterance_slot[_slot['slot']].append(turn['utterance'][_slot['start']:_slot['exclusive_end']].lower())
                            string_list.append((_slot['start'], _slot['exclusive_end']))

                        if not check_stop_words(utterance_slot, turn['utterance'].lower(), string_list, FLAGS.stop_word_path):
                            continue
                        expression = Expression(turn['utterance'].lower(), frame['state']['active_intent'], utterance_slot, string_list)
                        intent_templates[frame['state']['active_intent']].add_sample(expression)
                        sentence_set[frame['state']['active_intent']].add(expression.utterance)
                    pre_intents = active_intents
    return intent_templates


class SearchSimilarExpressions:
    """
    using sentence-transformer to encode all the utterance with new intent
    """

    def __init__(self, intent_expressions):
        self.expression_corpus = []  # expression corpus used to be encoded by bert for all expressions
        self.idx2expression = {}  # map idx to expression object
        self.intent_range = {}
        self.sentence_embeddings = None
        self.tfidf_matrix = None
        idx = 0
        stt = 0
        for intent, expressions in intent_expressions.items():
            end = len(expressions)
            # give the range of the expressions in the expression_corpus for every intent,  left closed right open
            self.intent_range[intent] = (stt, stt + end)
            stt += end
            for expression in expressions:
                self.expression_corpus.append(expression.utterance)

                expression.idx = idx
                # given the index for the order of the expression,idx indicates the order of the
                # sentence in the total expressions
                self.idx2expression[idx] = expression
                idx += 1

        idf_vectorizer = TfidfVectorizer(use_idf=True)
        self.tfidf_matrix = idf_vectorizer.fit_transform(self.expression_corpus).toarray()
        self.sentence_embeddings = model.encode(self.expression_corpus)


def dataset_type(train_percentage, dev_percentage):
    val = random.random()
    if val < train_percentage:
        return TRAIN
    elif val < (train_percentage + dev_percentage):
        return DEV
    return TEST


class IntentExample:
    def __init__(self, quadruple, exemplar=True):
        self.type = "intent"
        self.kind = "exemplar" if exemplar else "description"
        self.source = quadruple[0]
        self.label = quadruple[1]
        self.utterance = quadruple[2]
        self.exemplar = quadruple[3]

    def toJSON(self):
        return json.dumps(self, default=lambda o: o.__dict__, sort_keys=True, indent=4)


class IntentExampleGenerator:
    """
    generate examples
    """

    def __init__(self, training_percentage, neg_percentage, intent_template_dict, seed=None):
        if training_percentage < 0.0 or training_percentage > 1.0:
            raise ValueError("training_percentage is out of range")
        self.neg_percentage = neg_percentage
        self.training_percentage = training_percentage
        self.seed = seed
        self.intent_template_dict = intent_template_dict

    def __call__(self, expressions):

        examples = defaultdict(list)
        random.seed(self.seed)
        starttime = time.time()
        SSE = SearchSimilarExpressions(expressions)
        expression_corpus = SSE.expression_corpus
        intent_range = SSE.intent_range
        idx2expression = SSE.idx2expression

        if FLAGS.decode_method == 'tfidf':
            embed_matrix = SSE.tfidf_matrix.astype('float32')
        else:
            embed_matrix = SSE.sentence_embeddings.astype('float32')
        embed_dim = embed_matrix.shape[1]
        dim, measure = embed_dim, faiss.METRIC_INNER_PRODUCT
        param = 'Flat'
        index = faiss.index_factory(dim, param, measure)

        index.add(embed_matrix)

        total_positive_cnt = 0
        total_negative_cnt = 0
        for intent, range_ in tqdm(intent_range.items()):
            intent_sample = []
            entire_positive_sample = []

            for i in tqdm(range(range_[0], range_[1])):
                for j in range(range_[0], range_[1]):
                    # if i != j:#is add equal sent,we need to delete it
                    if FLAGS.cover_filter == "True":
                        if cover_reaction(idx2expression[i], idx2expression[j]):
                            if FLAGS.random_generate == 'True':
                                equal_sent = self.intent_template_dict[idx2expression[i].intent].generate_utterance(
                                    idx2expression[i])
                                pair = [intent, "1", equal_sent, idx2expression[j].exemplar]
                            else:
                                pair = [intent, "1", expression_corpus[i], idx2expression[j].exemplar]

                            entire_positive_sample.append(json.dumps(IntentExample(pair)))
                    else:
                        if FLAGS.random_generate == 'True':
                            equal_sent = self.intent_template_dict[idx2expression[i].intent].generate_utterance(
                                idx2expression[i])
                            pair = [intent, "1", equal_sent, idx2expression[j].exemplar]

                        else:
                            pair = [intent, "1", expression_corpus[i], idx2expression[j].exemplar]
                        entire_positive_sample.append(json.dumps(IntentExample(pair).toJSON(), indent=4))
            entire_positive_sample = list(set(entire_positive_sample))

            if len(entire_positive_sample) < FLAGS.pos_num or FLAGS.pos_num == -1:
                subsample_pos = entire_positive_sample
            else:
                subsample_pos = random.sample(entire_positive_sample, FLAGS.pos_num)
            positive_cnt = len(subsample_pos)
            for item in subsample_pos:
                partition = dataset_type(FLAGS.training_percentage, FLAGS.dev_percentage)
                examples[partition].append(item)
                intent_sample.append(item)

            entire_negative_sample = []
            if FLAGS.cover_filter == "True":
                k = int((range_[1] - range_[0]) * 1.3)
            else:
                k = int((range_[1] - range_[0]) * 1.7)
            D, I = index.search(embed_matrix[range_[0]:range_[1]], k)
            for i in tqdm(range(range_[0], range_[1])):
                topkidx = I[i - range_[0]]
                similar_neg_ex_idx = topkidx[((topkidx < range_[0]) | (topkidx >= range_[1])) & (topkidx != -1)]
                for idx in similar_neg_ex_idx:
                    neg_expression = idx2expression[idx]
                    neg_expression_utterance = neg_expression.utterance
                    neg_expression_intent = neg_expression.intent
                    if FLAGS.random_generate == 'True':
                        equal_sent = self.intent_template_dict[idx2expression[i].intent].generate_utterance(
                            idx2expression[i])
                        pair = [intent + "_" + neg_expression_intent, "0", equal_sent, neg_expression.exemplar]
                    else:
                        pair = [intent + "_" + neg_expression_intent, "0", expression_corpus[i],
                                neg_expression.exemplar]
                    entire_negative_sample.append(json.dumps(IntentExample(pair).toJSON(), indent=4))

            entire_negative_sample = list(set(entire_negative_sample))
            if len(entire_negative_sample) < FLAGS.neg_num or FLAGS.neg_num == -1:
                subsample_neg = entire_negative_sample
            else:
                subsample_neg = random.sample(entire_negative_sample, FLAGS.neg_num)
            negative_cnt = len(subsample_neg)
            for item in subsample_neg:
                partition = dataset_type(FLAGS.training_percentage, FLAGS.dev_percentage)
                examples[partition].append(item)
                intent_sample.append(item)

            print(intent, "(pos:", positive_cnt, "neg:", negative_cnt, ")")
            total_positive_cnt += positive_cnt
            total_negative_cnt += negative_cnt
        print(
            f'total_positive_cnt :{total_positive_cnt} total_negative_cnt :{total_negative_cnt}  total:{total_positive_cnt + total_negative_cnt} ')
        endtime = time.time()
        print("total Time ", (endtime - starttime))
        return examples


def save(intent_examples, path, label):
    """
    save generated examples into tsv files
    :param intent_examples:  generated examples
    :param path: output path
    :return: None
    """
    for key, examples in intent_examples.items():
        # we only generate one file for each folder
        with open(os.path.join(path, label), 'w', encoding='utf-8') as f:
            for example in examples:
                f.write(example + "\n")
    return


@gin.configurable
class ModelEncoder:
    def __init__(self, model_ckpt):
        self.device = "cuda:0" if torch.cuda.is_available() else "cpu"
        self.model = model_ckpt
        self.tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
        self.model = AutoModel.from_pretrained(model_ckpt).to(self.device)

    def convert(self, text_list):
        encoded_input = self.tokenizer(text_list, padding=True, truncation=True, return_tensors="pt").to(self.device)
        encoded_input = {k: v for k, v in encoded_input.items()}
        return self.model(**encoded_input).last_hidden_state[:, 0]


def query(encoder, text, dataset, k=4):
    embedding = encoder.convert([text])[0].detach().cpu().numpy()
    scores, samples = dataset.get_nearest_examples("embeddings", embedding, k=k)
    samples_df = pd.DataFrame.from_dict(samples)
    samples_df["scores"] = scores
    samples_df.sort_values("scores", ascending=False, inplace=True)
    return samples_df


def get_exemplar_dataset(templates, encoder):
    source = []
    exemplars_list = []

    for key in templates.keys():
        exemplars = templates[key]
        for exemplar in exemplars.exemplars:
            source.append(key)
            exemplars_list.append(exemplar)

    results = pd.DataFrame({"source": source, "text": exemplars_list})
    results = Dataset.from_pandas(results)
    results = results.map(
        lambda x: {"embeddings": torch.nn.functional.normalize(encoder.convert([x["text"]])).detach().cpu().numpy()[0]}
    )

    results.add_faiss_index(column="embeddings")
    return results


def printDF(results):
    for _, row in results.iterrows():
        print(f"{row['source']}:{row['text']}={row['score']}")


if __name__ == '__main__':
    gin.parse_config_file(sys.argv[1])

    FLAGS = SgdDataset()

    encoder = ModelEncoder()

    descriptions_dataset = load_description_dataset(FLAGS.input)

    templates = load_intent_meta(FLAGS.input)

    for key in templates:
        templates[key].finalize()

    # now we can create intent examples.
    



    #build_intent_examples = IntentExampleGenerator(
    #    FLAGS.training_percentage, FLAGS.negative_proportions, intent_template_dict)

    #if FLAGS.fix_random_seed:
    #    build_intent_examples.seed = 202006171752
    #intent_examples = build_intent_examples(intent_expressions)
    #save(intent_examples, FLAGS.output, FLAGS.label())
